\section{Capabilities of Information Networks}
\label{section:capabilities}
When it comes to managing the usage of information resources,  information-centric networks provide capabilities that traditional packetized networks cannot.  The basic structure of packet networks facilitates simple and efficient data transfer, but is fundamentally based on certain design assumptions that render network-centric usage management difficult at best and impossible at worst ~\cite{Clark:1995:DPD:205447.205458,SaReCl:84}.  Information-centric networks, taking a very different approach to network design, are much more amenable to embedded content control based on their different design principles as discussed in Section ~\ref{section:information-centric-networking}.

Current packet-based systems share three underlying design principles.  Strict layering, in which upper layers only use services that exist in lower layers which in turn have no knowledge of upper layers, end-to-end arguments governing service placement, and limited runtime packet sizes.  All three of these increase the difficulty in applying control over information transmitted through networks.

In internet systems, switching and routing traditionally occur in the lower layers of the OSI model ~\cite{Tanenbaum:1985:CN:536716}.  These decisions are made based on a priori knowledge of a given network topology, by manual or programmatic configuration ~\cite{proposal:openflow} and are not impacted by transmitted content except in very high-end systems ~\cite{cisco-6500}.  In fact, access to application content occurs at much higher levels ~\cite{Tanenbaum:1985:CN:536716}.  As a result of strict service layering, the information needed to make content-sensitive routing decisions is simply not available without breaking layer encapsulation on these kinds of devices.  Granted, Vendors do provide switches that examine application-level traffic ~\cite{cisco-6500}.  These intelligent switches are expensive however and as a result are only feasible for large ISPs to deploy. 

End-to-end arguments dictate where services should be placed in a network.  Services like information distribution control that require access to application layer data should, following these principles, be deployed into the ends of a given network ~\cite{SaReCl:84}.  Admittedly, this does encourage scalable network design, by keeping the core of a network simple, efficient, and fast. It however does not support granular information distribution control based on content rather than topology.  In order to control information flow based on content, internal network nodes must be able to access and evaluate transmitted content.  The fundamental end-to-end principles when applied to this problem would strictly prohibit that kind of content analysis.

Likewise, policies associated with content can be arbitrarily large.  As a result, they can exceed maximum packet sizes defined in packetized networks.  Furthermore, as content-sensitive networks must evaluate defined policies prior to routing content, any policy to be evaluated must be completely downloaded into a router and analyzed for suitability for transmission prior to any packet routing, leading to inevitable bottlenecks as content is queued behind the policy elements.

Content analysis of certain kinds of transmitted artifacts may not be possible without a holistic perspective either.  For example, if an XML document is transmitted through a network, that document may very well have content in element $n$ which is described in more detail in element $n+2$.  Here, element $n$ and element $n+2$, by themselves, are not sensitive.  When combined however, they are.  When transmitted, these elements would be in separate packets.  For the sake of this argument, assume these packets are built such that element $n$ is in packet $m$, and element $n+2$ is in packet $m+c$, where $c$ is some constant, and that packet $m$ is assembled and transmitted from the source node at some time prior to packet $m+c$.  In this scenario, packet $m$ will be passed through an intervening nodes prior to packet $m+c$.  Even nodes that maintain a history of transmitted content that may be able to determine that information in $m$ is sensitive when combined with information in $m+c$ will be unable to undo the earlier transmission of packet $m$.  In order to circumvent this problem, nodes would need to hold packets for some time $t$ to check for context.  This may help solve the problem, as related information likely has some kind of intrinsic locality, but nevertheless the size of $c$ can be still be relatively arbitrary.  As a result, the size of $t$ is impossible to set {\it a priori}.  This approach imposes possibly significant performance penalties as well.

Information-centric networks are based on different primitives, as described in Section ~\ref{section:information-centric-networking}.  Specifically, they are based on named data objects with strict name-data integrity, as well as other associated principles.  This different abstraction makes policy evaluation and content binding simpler, as content can be bound either in-line to policy or via specific naming conventions.  In these systems, once content is located by name, it is returned to the requester either via a predefined path mirroring the original request path or a variable response path.  In either case however, all content and associated policy is available at each routing node in that return path, and can be evaluated for suitability of transmission.

As a result of these fundamentally different underlying models, information-centric networks in the next-generation internet enable usage management capabilities that are very problematic to implement and enforce in current internet architectures.