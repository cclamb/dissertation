\section{Taxonomies of Usage Management Overlay}
A clear taxonomic organization of potential steps in approaching finer grained policy based usage management helps in describing the difficulties inherent in developing potential solutions as well as aiding in planning system evolution over time. Here, we have five distinct types of integrated policy-centric usage management systems, as shown in Table \ref{table:model:taxonomy}.  Of these five, only the first two levels are represented in current system model.

\begin{table*}[tp] %
\centering %
\begin{tabular}{clcc}
\toprule %
$ Name$ 	& $Description$ \\\toprule %
$\phi$ 		& The initial level of this taxonomy, $\phi$ classified systems \\
 			& have a single guard without policy-based control \\\midrule
$\alpha$	& $\alpha$ classified systems have a single guard by have begun \\
			& to integrate policy-based control \\\midrule
$\beta$		& Systems that have begun to integrate policy-based control with \\
			& router elements are in the $\beta$ category \\\midrule
$\gamma$	& Systems that have integrated policy-based control with routing \\
			& and computational elements \\\midrule
$\delta$	& Continuous policy-based control with \textit{smart licensed} artifacts \\\bottomrule
\end{tabular}
\caption{Proposed Usage Management Taxonomy}
\label{table:model:taxonomy}
\end{table*}

In this taxonomy, it is not required that systems pass through lower levels to reach higher ones.  This taxonomy represents a continuum of integration of usage management controls.  Systems can very well be designed to fit into higher taxonomic categories without addressing lower categories.  That said however, many of the supporting infrastructural services, like identification management or logging and tracing systems, are common between multiple levels.

The taxonomy itself starts with the current state, integrating policy evaluation systems into the network fabric gradually, moving away from filters, then by adding policy evaluation into the routing fabric, then the computational nodes, and finally by incorporating evaluation directly into content.

\subsection{Evaluation Methodology and Model}
In order to successfully evaluate the elements of our overlay taxonomy, we must first establish a model against which to measure the presented architectures.  The current standard for evaluating software quality is ISO/IEC 25020 and this, along with other related standards from other service delivery organizations has begun to be integrated into both academia and industry as a tractable way to measure system quality \cite{5958158, proposal:iso-25020}.

This particular model must address quality attributes specific to the presented architectures rather than the functional domain.  The goal of this model is to allow for architectural evaluation of policy evaluating architectures regardless of the specific functional domain.  Ergo, injecting a specific functional domain into the evaluation or the evaluating model is unacceptable.  Acceptable attributes are those which directly target quality attributes of the architectures in question.

\begin{eqnarray}
E = \lbrace f_{e}, f_{r}, f_{u}, f_{p}, f_{m}, f_{f}, f_{s}, f_{c} \rbrace \\
W= \lbrace w_{e}, w_{r}, w_{u}, w_{p}, w_{m}, w_{f}, w_{s}, w_{c} \rbrace \\
s = \sum_{W, E} w_{i} f_{i}
\end{eqnarray}

We are specifically interested in evaluating architectures for policy evaluation functional suitability, reliability, usability, possible performance efficiencies, maintainability, portability, security, and compatibility, specifically neglecting any kind of domain functional suitability.  Each area will be associated with an evaluation function.  The suitability of a given architectural option will be evaluated by a tuple of these functions, which can then be converted into a weighted sum leading to a single quantitative metric representing suitability under evaluated conditions.

Also important to note, certain attributes may not be able to be evaluated using specific architectural models.  For example, conceptual or notional architecture models are intended to convey specific ideas prevalent in a given system architecture rather than ways that architecture will be realized.  As such, these kinds of models generally cannot be evaluated for things like portability or performance efficiency, as these qualities usually manifest based on specific standard and technology selections, respectively.  In these cases, the evaluation functions representing those attributes will be weighted at zero.

\subsubsection*{Unevaluated Attributes}
Functional suitability in this context is reflects the ability of the system to accurately manage artifacts based on policy and context.  As the functional domain is the same for all examined systems, we will neglect this and set $ w_{e} = 0 $.

These are purely notional models, and as a result cannot be evaluated using generally accepted usability metrics like the Systems Usability Scale (SUS) \cite{proposal:sus}.  Ergo, for this analysis we will set the weight $w_{u} = 0$.

Performance efficiency is generally a characteristic of logical and physical system architectures.  Specific logical architectures can certainly decrease the performance of a given system through poor design, insufficient caching, badly considered state management, or inferior scalability.  Physical architectures clearly impact performance if processing power, storage, communication bandwidth, or other attributes are insufficient to process apparent loads.  We will therefore set $w_{p} = 0$ in this analysis.

One way to evaluate system portability is via standard compliance \cite{5958158}.  Proposed architectures herein will not be evaluated to the level at which proposed standards have an impact.  Therefore, in this analysis, $ w_{f} =0 $.

Compatibility with existing systems requires either accepted system standards or other systems with which to be compatible.  In this analysis, we have neither accepted standards or other systems, so we will set $ w_{c} = 0 $.

\subsubsection*{Evaluated Attributes}
In the scope of this paper, we will evaluate system reliability by evaluating the information entropy of a given system in which failure is modeled using a Weibull distribution over selected systems elements emulating a burn-in model with faults decreasing over time\cite{5754490}.  The function $ f_{r} $ for a given architecture will be a functional representation of the entropy of the component distributions.

Information entropy here is used to succinctly evaluate the stability of a given system component.  In this context, the higher the entropy, the more unstable a given system is as the randomness inherent in that system leads to a higher required communication bandwidth to enumerate a given possible state \cite{cover:book91}.  This approach is also more succinct for this context as it requires less specific knowledge with respect to precise component behavior under load when compared to other approaches like Bayesian Networks and is more quantitative than fault tree analysis \cite{Rausand-Hoyland-2003}.  Other software specific models require either \textit{a priori} knowledge of components or neglect the risk associated with higher complexity imparted by additional system componentry \cite{5305674,5368230}.  

Maintainability can be measured via examination of a given system with an eye toward areas prone to change.  Loosely coupled components directly contribute to the ability to change a specific system component.  With this in mind, we can measure develop a simple ratio to indicate the maintainability of a given system --- the total number of components with high change impact to the number of components with high change impact that have been decoupled from the system via an interface 

\begin{equation}
f_{m} = c_{i} / c_{\delta}
\end{equation}

We will evaluate security of a given system via attack surface analysis of the system itself as well as the protected domains.

Any weights not explicitly set to zero previously will be set to one, giving us:

\begin{equation}
W = \lbrace 0, 1, 0, 0, 1, 0, 1, 0 \rbrace
\end{equation}

or, in essence:

\label{equation:final}
\begin{equation}
s = f_{r} + f_{m} + f_{s}
\end{equation}

\subsection{$\phi$-level Overlay Systems}
The $\phi$ classification consists of systems like the initial NSA and BAH notional models.  These systems consist of two distinct domains, separated by a filter-centric single guard.  The initial NSA system model is clearly of this type, separating two domains with a guard using filter chains.  The BAH model is also of this type, using a Filter Segment to evaluate data packages transmitted between interface segments attached to specific domains.

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{model-phi-crop}
\caption{Taxonomy ($\phi$)}
\label{fig:model:taxonomy-phi}
\end{figure}

Generally one of the domains supports more sensitive information than the other, but that is not always the case.  In the models we have examined this has certainly been true, but classified information for example  is commonly stored in \textit{compartments} which are separated by clear \textit{need-to-know} policies enforced by access lists and classification guides.  These kinds of compartments contain information at similar levels of classification, but contain distinct informational elements that should not be combined.

In these kinds of systems, specific rules regarding information transfer and domain characterization are tightly bound to individual filter implementations.  They are based on \textit{a priori} knowledge of the domains the guard connects, and therefore are tightly coupled to the domains they connect.  Furthermore, the filter elements are standalone within the system, in this classification, not availing themselves of external resources.  Rather, they examining information transiting through the filter based purely on the content of that information.

\subsection{$\alpha$-level Overlay Systems}
The $\alpha$ overlay classification contains systems that have begun to integrate policy-centric usage management.

Both policies and contexts are dynamically delivered to the system.

The dynamic delivery of context and policies allows these kinds of systems more flexibility with policy evaluation.

The $\alpha$ category begins to integrate policy-centric management rather than using strict content filtering.

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{model-alpha-crop}
\caption{Taxonomy ($\alpha$)}
\label{fig:model:taxonomy-alpha}
\end{figure}

Here, we again have at least two domains, Domain A and Domain B, though we could potentially have more.  $\phi$ type systems require domain specific information to be tightly coupled to the filter implementations.  Separating the permissions, obligations, and other constraints from the filters and incorporating them into a specific separate policy entity frees the Guard from this coupling and provides additional flexibility to the system.

The guard can continue to use filters to process data.  These filters however are now more generic and decoupled from the specific domains it manages.  The choice of using a specific filtering model rather than some other kind of construct is a design detail level to implementers.  That said however, individual filters will be remarkably different and still need to understand the ontologies over which specific licenses are defined.

The policy repository is key to the implementation and differentiation of this taxonomy category.  This repository can be implemented as a separate repository keyed into via a data artifact's unique URI, for example.  It could also represent a policy sent in tandem with a data artifact in a data package.

The policy repository may be implemented as some kind of external service, and as such, represents the first such external service explicitly used in this taxonomy.  Other external services may well exist and be used to adjudicate information transfer decisions as well.

\subsection{$\beta$-level Overlay Systems}
The $\beta$ taxonomic category begins to integrate policy-centric processing with router elements in a given network.  While this work is centered on using overlay technology to illustrate and implement these concepts, it is important to note that this kind of distributed policy-centric processing could very well be distributed into the physical routing fabric of a given network as well by extending Software Defined Networking systems like OpenFlow \cite{proposal:openflow}.

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{model-beta-crop}
\caption{Taxonomy ($\beta$)}
\label{fig:model:taxonomy-beta}
\end{figure}

In this model we can also host multiple domains as a result of flexible policy-based content examination.  Each domain hosts a network of some kind, though that hosted network could very well be a degenerate network of a single system.  Each network hosted in a domain is hierarchical, with specific computational nodes embodied by workstations, tablet computers or mobile devices, and routing points embodied by routers or switches of some kind.

Policy evaluation in this model has begun to penetrate into the routing elements of the specific domain networks.  Here, note that we have started to penetrate into the routing fabric of the network by doing content evaluation at router points.  Content-based switching networks have been successful in other domains, and such techniques can be used here to provide policy evaluation capabilities.  

Certain types of traffic are easier to evaluate than others however.  For example, HTTP requests and responses are easier to examine that TCP packets.  When examining TCP packets, systems generally require additional context to select an appropriate packet window (e.g. the number of packets cached for examination).  HTTP traffic does not usually require this kind of flexibility.

This migration of policy evaluation into the routing fabric provides for enhanced data security and better network management, especially if part of a network is compromised.  Now that policy decisions can be made at the router level in a given network, we are starting to have network security in depth rather than simple perimeter protection.  This not only provides the ability for additional information protection, but also allows for different compartments holding information at different need-to-know levels to be created ad-hoc under different routing segments.  In cases of network compromise, this kind of dynamic policy enforcement can also allow for quick node excision as well.

\subsection{$\gamma$-level Overlay Systems}
The $\gamma$ compartment has integrated policy evaluation with compute and routing nodes.  Here, policies can be evaluated against content at all network levels --- nodes emitting requests, nodes fielding requests, and all routing elements in between.

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{model-gamma-crop}
\caption{Taxonomy ($\gamma$)}
\label{fig:model:taxonomy-gamma}
\end{figure}

We see that the policy repository is supplying services to all computational elements in both domains.  This gives us increased granularity with respect to data compartmentalization by integrating information security into each network element.  At this point, the network can create compartments of single nodes, while previously in $\beta$ level systems compartments could only be created under specific routing elements.  At this level, we can also provide services revoking data access based on policy evaluation decisions when needed.

Furthermore, individual node exclusion is possible as well. $\beta$ classified systems could excise network elements under specific routers by dynamic policy application.  Now, we can apply the same functionality to individual compute nodes.  For example, if a networked device like a smart phone is compromised, that device can be removed from access quickly or used to feed mis-information to adversaries.

\section{Taxonomic Analysis}
Now that we have developed a quality model and defined the taxonomic elements describing a migration from essentially no usage management to pervasive usage management, we can begin to apply that model to the elements themselves.  We will begin by applying the model to $\phi$ elements, proceeding through the taxonomy to $\gamma$ elements.  In each case, we will describe the notional architecture including any inferred services required for application of our usage management model.  Then we will apply the quality model from \ref{equation:final}.  Finally, we will compare results of the analysis and extract implications with respect to appropriate conditions for specific architecture application.

In each analysis, a failure is either not passing through acceptable content, or passing through unacceptable content. Furthermore, handing content to review of any kind is considered a failure if that content is acceptable for transmittal, and a success otherwise.

\subsection{$\phi$ Classification Analysis}
$\phi$ classified systems are remarkably simple and self-contained, but not invulnerable to failure, and can be very difficult to maintain.

A typical system in this classification receives content, with associated meta-data, passes that content to a chained group of filters, which then pass the content onto a dispatcher if it passes filter evaluation.  If it does not pass filter evaluation, it is then held for future review.

% Reliability

% Maintainability
From a maintainability perspective, immediate points of possible volatility include filter components, receptor components, and dispatcher components.  Filters are generally expected to be the most vulnerable to required change as stakeholders create additional filter rules that must be deployed.  Filters also require replacement as meta-data formats change.  Content receivers and dispatchers may need to change as well based on content format changes or content dispatch rules.

In these kinds of systems, none of these components are designed with appropriate interfaces supporting easy upgrade at this level of evaluation.  Ergo, in this case, $ f_{m} = 0 $.

% Security


\subsection{$\alpha$ Classification Analysis}

\subsection{$\beta$ Classification Analysis}

\subsection{$\gamma$ Classification Analysis}