\section{Taxonomies of Usage Management Overlay}
A clear taxonomic organization of potential steps in approaching finer-grained policy based usage management helps in describing the difficulties inherent in developing potential solutions as well as aiding in planning system evolution over time. Here, we have four distinct types of integrated policy-centric usage management systems, as shown in Table ~\ref{table:model:taxonomy}.  Of these four, only the first two levels are represented in current system models.

\begin{table*}[tp] %
\centering %
\begin{tabular}{clcc}
\toprule %
$ Name$ 	& $Description$ \\\toprule %
$\phi$ 		& The initial level of this taxonomy, $\phi$ classified systems \\
 			& have a single guard without policy-based control \\\midrule
$\alpha$	& $\alpha$ classified systems have a single guard by have begun \\
			& to integrate policy-based control \\\midrule
$\beta$		& Systems that have begun to integrate policy-based control with \\
			& router elements are in the $\beta$ category \\\midrule
$\gamma$	& Systems that have integrated policy-based control with routing \\
			& and computational elements \\\midrule
% $\delta$	& Continuous policy-based control with \textit{smart licensed} artifacts \\\bottomrule
\end{tabular}
\caption{Proposed Usage Management Taxonomy}
\label{table:model:taxonomy}
\end{table*}

In this taxonomy, it is not required that systems pass through lower levels to reach higher ones.  This taxonomy represents a continuum of integration of usage management controls.  Systems can very well be designed to fit into higher taxonomic categories without addressing lower categories.  That said however, many of the supporting infrastructural services, like identification management or logging and tracing systems, are common between multiple levels.

The taxonomy itself starts with the current state, integrating policy evaluation systems into the network fabric gradually, moving away from filters, then by adding policy evaluation into the routing fabric, then the computational nodes, and finally by incorporating evaluation directly into content.

The UCDMO, described previously, is focused exclusively on promoting controlled sharing of sensitive information and has specific goals that a clear, realized taxonomy of granular information-centric usage management helps fulfill.  Those goals include an ideal end state described as a flat network architecture with usage management injected into the distributed system.  This is exactly the final $\gamma$ architecture described within this taxonomy ~\cite{proposal:cd101,proposal:ucdmo-goals}.  The UCDMO also has specific goals outlined within its founding charter, including:

\begin{itemize}
\item \textbf{Optimize Capabilities} ---  Drive robust and extensible cross domain capabilities to support a secure and integrated information enterprise.
\item \textbf{Oversee Resources} ---  Maximize return on cross domain investments, reduce duplication of effort, and increase efficiency of cross domain activities.
\item \textbf{Mitigate Risk} ---  Support risk-based decisions by enabling global awareness of cross domain operational connections.
\item \textbf{Provide Leadership} ---  Provide leadership across the inter-agency spectrum to ensure coordinated cross domain governance, oversight and community reciprocity.
\end{itemize}

Our work here certainly contributes to these goals, providing robust cross-domain capabilities, helping mitigate risk, and contributing toward advancing the state of the art in this kind of multi-level security environment.

\subsection{$\phi$-level Overlay Systems}
The $\phi$ classification consists of systems like the initial NSA and BAH notional models.  These systems consist of two distinct domains, separated by a filter-centric single guard.  The initial NSA system model is clearly of this type, separating two domains with a guard using filter chains.  The BAH model is also of this type, using a Filter Segment to evaluate data packages transmitted between interface segments attached to specific domains.

\begin{figure}[!t]
\centering
\includegraphics[width=4in]{model-phi-crop}
\caption{Taxonomy ($\phi$)}
\label{fig:model:taxonomy-phi}
\end{figure}

Generally one of the domains supports more sensitive information than the other, but that is not always the case.  In the models we have examined this has certainly been true, but classified information for example  is commonly stored in \textit{compartments} which are separated by clear \textit{need-to-know} policies enforced by access lists and classification guides.  These kinds of compartments contain information at similar levels of classification, but contain distinct informational elements that should not be combined.

In these kinds of systems, specific rules regarding information transfer and domain characterization are tightly bound to individual filter implementations.  They are based on \textit{a priori} knowledge of the domains the guard connects, and therefore are tightly coupled to those domains.  Furthermore, the filter elements are standalone within the system, in this classification, not availing themselves of external resources.  Rather, they examine information transiting through the filter based purely on the content of that information.

The set of filters that could be developed and deployed within the guard are unlimited.  Developers could easily create a filter that inspects and possibly redacts the sections within the document, rather than passing or not passing the entire document through the guard.  Indeed, if we assume even very limited processing capabilities within the guard, that is, turing completeness, then this guard can be made as powerful as any solution we can derive for implementing a cross-domain solution (CDS). Thus the computational power of the guard is not the issue. The real issues are the benefits that can be gained by distributing the capabilities intelligently within the networked environment as opposed to fixing them programmatically and topologically at the perimeter of a sensitive network.

\subsection{$\alpha$-level Overlay Systems}
The $\alpha$ overlay classification contains systems that have begun to integrate policy-centric usage management. Both policies and contexts are dynamically delivered to the system. The dynamic delivery of context and policy allows these kinds of systems more flexibility with policy evaluation. The $\alpha$ category begins to integrate policy-centric management rather than using strict content filtering.

\begin{figure}[!t]
\centering
\includegraphics[width=4in]{model-alpha-crop}
\caption{Taxonomy ($\alpha$)}
\label{fig:model:taxonomy-alpha}
\end{figure}

Here, we again have at least two domains, Domain A and Domain B, though we could potentially have more.  $\phi$ type systems require domain specific information to be tightly coupled to the filter implementations.  Separating the permissions, obligations, and other constraints from the filters and incorporating them into a specific separate policy entity frees the guard from this coupling and provides additional flexibility to the system.

The guard can continue to use filters to process data.  These filters however are now more generic and decoupled from the specific domains the guard manages.  The choice of using a specific filtering model rather than some other kind of construct is a design detail level to implementers.  That said however, individual filters will be remarkably different and still need to understand the ontologies over which specific licenses are defined rather than specific content semantics.

The policy repository is key to the implementation and differentiation of this taxonomy category.  This repository can be implemented as a separate repository keyed into via a data artifact's unique URI, for example.  It could also represent a policy sent in tandem with a data artifact in a data package.

The policy repository may be implemented as some kind of external service, and as such, represents the first such external service explicitly used in this taxonomy.  Other external services may well exist and be used to adjudicate information transfer decisions as well.

\subsection{$\beta$-level Overlay Systems}
The $\beta$ taxonomic category begins to integrate policy-centric processing with router elements in a given network.

\begin{figure}[!t]
\centering
\includegraphics[width=4in]{model-beta-crop}
\caption{Taxonomy ($\beta$)}
\label{fig:model:taxonomy-beta}
\end{figure}

In this model we can also host multiple domains as a result of flexible policy-based content examination.  Each domain hosts a network of some kind, though that hosted network could very well be a degenerate network of a single system.  Each network hosted in a domain is hierarchical, with specific computational nodes embodied by workstations, tablet computers or mobile devices, and routing points embodied by routers or switches of some kind.

Note that usage management has started to penetrate into the routing fabric of the network by doing content evaluation at router points.  Content-based switching networks have been successful in other domains, and such techniques can be used here to provide policy evaluation capabilities ~\cite{proposal:jboss-esb}.  

Certain types of traffic are easier to evaluate than others however.  For example, HTTP requests and responses are easier to examine that TCP packets.  When examining TCP packets, systems generally require additional context to select an appropriate packet window (e.g. the number of packets cached for examination).  HTTP traffic does not usually require this kind of flexibility.  Information-centric networks, due to their shift from previous network models, follow this same kind of pattern by placing the data to be protected at the center of attention.

This migration of policy evaluation into the routing fabric provides for enhanced data security and better network management, especially if part of a network is compromised.  Now that policy decisions can be made at the router level in a given network, we are starting to have network security in depth rather than simple perimeter protection.  This not only provides the ability for additional information protection, but also allows for different compartments holding information at different need-to-know levels to be created ad-hoc under different routing segments.  In cases of network compromise, this kind of dynamic policy enforcement can also allow for quick node excision as well.

\subsection{$\gamma$-level Overlay Systems}
The $\gamma$ compartment has integrated policy evaluation with compute and routing nodes.  Here, policies can be evaluated against content at all network levels --- nodes emitting requests, nodes fielding requests, and all routing elements in between.

\begin{figure}[!t]
\centering
\includegraphics[width=4in]{model-gamma-crop}
\caption{Taxonomy ($\gamma$)}
\label{fig:model:taxonomy-gamma}
\end{figure}

We see that the policy repository is supplying services to all computational elements in both domains.  This gives us increased granularity with respect to data compartmentalization by integrating information security into each network element.  At this point, the network can create compartments of single nodes, while previously in $\beta$ level systems compartments could only be created under specific routing elements.  At this level, we can also provide services revoking data access based on policy evaluation decisions when needed.

Furthermore, individual node exclusion is possible as well. $\beta$ classified systems could excise network elements under specific routers by dynamic policy application.  Now, we can apply the same functionality to individual compute nodes.  For example, if a networked device like a smart phone is compromised, that device can be removed from access quickly or used to supply mis-information.

\section{Taxonomic Analysis}
The various levels of the taxonomy vary primarily with respect to the inclusion of policy-based usage management and information-centric structure.  $\phi$ type systems are not structured with information-centric use in mind, nor do they use policy-centric management.  Conversely, $\gamma$ type systems are both purely policy oriented and completely information-centric.

As systems move through the various levels of the taxonomy they gradually move from one side of the spectrum to another.  Information-centric structures, hierarchical or otherwise, gradually migrate into the network beginning with $\beta$ systems.  Policy orientation is injected into the architectures starting with $\alpha$ systems and moving into the network fabric in parallel with information-centric exploitation.

\subsection{Characteristics of Policy-centricity}
In these systems, policy-based management supplies distinct advantages over filter-centric information control.  This kind of policy-centric usage management is more content specific than filters, more flexible, and is more expressive than filter-centric systems.

%\subsubsection*{Content Specific}
Filters, in filter-based systems, are not coupled to the content passing through the system.  Rather, they are usually tied to the characteristics of attached networks.  For some filters, that is not problematic.  Mal-ware filters, for example, are very general and do not need to have an understanding of filtered content and are not sensitive to that content at all, though they can be very sensitive to specific context.  This limitation does however prohibit filters from doing anything content specific.  Due to their deployment limitations, in that they are deployed to such a system via a process distinct from processing content, they are unable to use presented content or current dynamic context to influence information processing decisions.

Consider content $c$ impacted by a dynamic context $d$ where $d$ is defined in terms of the content itself, the person or system requesting that content, and the environment in which that request is made.  Here, only under certain specific environmental conditions is that requesting agent allowed access to the requested content.  Ergo, the decision to pass the content to the requester is based upon characteristics of the content related to dynamic changes within the environment.  A filter-centric solution contained within the $\phi$ level of the taxonomy is unable to change filter rules based on changes like new content or environmental alteration as a result of the static nature of the deployed filters.  A policy-based system, on the other hand, is able to express the content specific policy easily for more dynamic evaluation.  This kind of content focus makes implementation of Clark-Wilson integrity bounds easier as well.  The clear demarcation of data objects simplifies management of trusted procedures and managed objects ~\cite{ClaWil87}.

For example, if $c$ contains information that can only be accessed for a specific time period, a static filter based on evaluating content only cannot determine that the information in $c$ is no longer appropriate for dissemination after that time period ends.  That kind of evaluation requires meta-data associated with $c$ that specifically describes these time bounds and a dynamic contextual evaluator able to determine when that window of access has closed, functionality static filters do not demonstrate.

%\subsubsection*{Flexibility}
Policy-centric systems are more flexible than filter-based counterparts.  In a filter-based solution, the type of content that can be evaluated is tightly coupled to the filters installed.  If a given piece of content is new to a given filter-centric solution, that content cannot be appropriately examined and must be submitted for human review.  A policy-based system is designed to be more general.  When based upon a common ontology, the evaluation system can be very general with respect to its evaluation of a given policy ~\cite{JaHeLa:10}.  A general policy engine can handle a great variety of different content as long as the policies associated with that content correspond to known domain ontologies.  This generality leads to a greater amount of flexibility with respect to what can be expressed in a specific policy.

A filter is going to have a specific responsibility, like redacting sensitive words from a document, for instance.  In order for that filter to redact those sensitive words, it must have access to some kind of list of what those sensitive words are.  Remember, $\phi$ level systems use static filters, so that filter can only be updated when the filter itself is updated.  Now a policy-centric system on the other hand can have a policy associating sensitivity with various areas of content in a specific document.  In this case, all the system must do is understand the sensitivity described in the policy associated with the content, and can then redact that content if needed.  The ontology describing the areas of sensitivity will change more slowly that the possible content itself, leading to a more flexible maintainable system.

This is of course a simple example solvable by creating a dynamic list; the key point of the above example is that the specificity of the filters requires additional complexity in the filter system itself.  The generality of the policy-centric system allows the complexity to be more clearly expressed and contained within the policy file.

%\subsubsection*{Expressiveness}
While filters can process content at specific perimeter points, its lack of reach into a given network fabric limits the power a given filter can actually have over transmitted content.  A policy associated with content, when transmitted with content, can reference much more than the semantics of the protected content.  That policy can describe specifically, in detail, how that content can be used.  Filters cannot exercise that level of control.

Assume a distributed system with multiple filter points.  In this kind of system, information distribution can be controlled via deployed filters at a relatively fine level of granularity.  This kind of distribution control cannot influence the use of protected content however --- once that content is distributed, possessors are accorded full access.

Policy-enabled systems are not limited in this way.  Policies, when coupled with policy evaluation tools, can exercise control not only over distribution and routing, but also over use of distributed content at endpoints.

These advantages accrue in usage management systems as policy capabilities are propagated through the information-centric fabric.  Some of these advantages, like expressiveness, appear simply by beginning to use policies instead of filters.  The remaining two have more of an impact as additional policy-centric nodes combine to form a system suitable for cloud deployment, increasing their impact as they move from $\alpha$ to $\gamma$ types of systems.

\subsection{Information-centric Structure}
Information-centric integration exhibits clear advantages over single point perimeter systems as well.  Specifically, information-centric systems are more partition-able than perimeter solutions, enable content throttling, provide capabilities for dynamic content control, and allow content to be more traceable.

%\subsubsection*{Characteristics of Partition-ability}
Administrators typically deploy filter-based perimeter protection at strategic routing points on secure networks.  These kinds of networks are designed with specific regions of enhanced sensitivity separated by cross domain management systems regulating information flows ~\cite{proposal:nsa-arch,proposal:raytheon-arch,proposal:bah-arch}.  While sensible from the perspective of each protected region as a secure domain, this design thinking begins to fall apart when exposed to the very real threat of the malicious insider.  Boundary-centric information flow control is impossible to realistically achieve when the actual boundaries between malicious actors and system users is constantly in flux.  When a malicious actor can be anywhere within a system, boundaries are simply too dynamic to be realistically recognized.  In order to surmount this fluid system posture, designers must adopt a security in depth mindset.

Information-centric networks enable this kind of defence in depth via the possibility of partitioning.  A given information-centric system depending on the level of can partition the user space and by doing so decrease the attack surface available to a malicious insider.  $\phi$ and $\alpha$ level systems based on perimeter filters simply do not present this ability.  Systems beginning with $\beta$ provide the potential to create need-to-know cells of finer granularity up to $\gamma$ type systems in which cells can be created at the level of specific nodes.  These need-to-know cells serve to help quarantine possible intrusion into the sensitive distribution fabric if that fabric is compromised by helping isolate that system failure within a compromised cell.

For example, assume a hypothetical system with nine nodes connected along a single data plane within a prototypical secure network.  With perimeter defenses, if one of those nodes is compromised, a malicious actor can potentially begin to monitor communications traffic between all network nodes, effectively compromising the entire network.  In this same network, if designers partition the system into three cells of three nodes, a similar intrusion in one of those cells will effectively only compromise that cell, leaving the other two cells unaffected.  This decrease in possible targets for compromise effectively decreased the network attack surface from any give node by $\frac{2}{3}$, correspondingly increasing the security posture of the system.

%\subsubsection*{Content Throttling}
Perimeter located filter systems only have the opportunity to control sensitive traffic at that initial boundary.  Information located in repositories behind that boundary is not subject to control if it is retrieved by an agent also ensconced behind that same system boundary.  Granted, control can be exerted at the repository level, but in a system with more than one repository, this is of limited impact.

A partitioned cell-oriented system, on the other hand, provides greater opportunity for information monitoring and control.  The partitions provide additional potential control points requests must cross in order to access needed information.  Furthermore, less random cell design provides the capability to unify repositories, providing tight control of information dissemination.

Our hypothetical nine-node system, for example, provides no control over information dispatched from one of the contained nodes to other contained nodes in its initial design form.  There are no control points within that nine-node network at which to monitor and control information flow.  Partitioning that space into three three-node cells provides at least two potential control points for inter-cell requests at which information flow can be monitored.  In cases where a malicious insider is actively collecting and hoarding data for exfiltration, these additional control points give administrators the ability to automatically throttle the rate at which sensitive material can be accessed by users to increase the cost of data collection and increase the likelihood of agent discovery.

%\subsubsection*{Dynamic Content Control}
Singular perimeter solutions due to their lack of internal control points also forego the ability to provide dynamic content control.  Once information has traversed a given perimeter access point, it is no longer under the control of that point and can no longer be retrieved, accessed, monitored, or modified.  Solutions with internal control points can provide the ability to continually monitor and control disseminated information.

Within a given information-centric system, depending on that system structure, data can be more rigorously controlled.  $\beta$ and $\gamma$ systems provide the ability to dynamically change information access via contextual changes at a finer grained level than perimeter solutions can.  $\gamma$ systems can in fact provide the ability to retract information access on a per request basis.

This kind of control is especially useful in situations where external partners may temporarily need access to sensitive information for a specific short period of time, say during some kind of joint exercise or activity.  $\gamma$ and $\delta$ systems can provide that access only during the window of operation, and retract that access when that window closes.  This kind of use is common in joint military operations with coalition partners.

This kind of dynamic content control can easily implement Brewer and Nash access control ~\cite{Brewer89}.  Access rules in policies can describe the general access to data object based on specific individual project context.  This project context could then be embodied in attributes associated with the user to authorize specific actions over objects.  In this way, a subject that has been granted access to information from project A can be dynamically denied access to content from project B, when projects A and B are mutually exclusive.

%\subsubsection*{Traceability}
The singular location of perimeter filter solutions also precludes easy information traceability.  Data requests within a given network sans internal controls is more difficult to trace than an information-centric solution with a partitioned cell structure that is tailored to the specific information requested (say, XML databases or semantic web content).  The partitioned information-centric system requires requests to traverse multiple routing nodes at which request and response content can be examined and stored for later analysis and visualization.  Perimeter solutions without this kind of structure cannot monitor flows at this finer-grained level.

The strengths of information-centric systems over single perimeter points gradually increase as information-centricity permeate any given system.  Some abilities, like content-centric access repudiation, can only occur at the $\gamma$ level.  Others, like traceability or throttling, become more effective as a system architecture traverses from lower to higher levels of capability within the proposed taxonomy.